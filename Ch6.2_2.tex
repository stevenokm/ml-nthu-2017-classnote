%% 1p
%
%\subsubsection{Cost Function}
%% 0.5P
%
%\subsubsubsection{Learning Conditional Distributions with Maximum Likelihood}
%% 1.25p
%
%\subsubsubsection{Learning Conditional Statistics}
%% 1.25p
%
%\subsubsection{Output Units}
%% 0.5p
%
%\subsubsubsection{Linear Units for Gaussian Output Distributions}
%% 0.5p
%
%\subsubsubsection{Sigmoid Units for Bernoulli Output Distributions}
%% 2.25p

\subsubsubsection{Softmax Units for Multinoulli Output Distributions}
% 3.25p
  Softmax functions are often used as a classifier, to classified $n$ different classes.

  We want to generalize the Bernoulli distribution to multinoulli distribution.

  % eq 6.27
  \begin{equation} \tag{6.27}
    \label{eq_6_27}
    \hat{y} = P( y=1\ |\ \bm{x} )
  \end{equation}

  The~\eqref{eq_6_27} shows a example of maximum likelihood defined by Bernoulli distribution.
  Next, using log-likelihood to simplify computation. We take $z _ i = \log \tilde{P}( y=1\ |\ \bm{x} )$, so:

  % eq 6.28
  \begin{equation} \tag{6.28}
    \label{eq_6_28}
    \bm{z} = \bm{W} ^ \top \bm{h} + \bm{b}
  \end{equation}

  and apply softmax function:

  % eq 6.29
  \begin{equation} \tag{6.29}
    \label{eq_6_29}
    \mathrm{softmax}{( \bm{z} )} _ i = \frac{\ \exp(z_i) } {\ \sum _ j \exp(z_j) }
  \end{equation}
  % eq 6.30
  \begin{equation} \tag{6.30}
    \label{eq_6_30}
    \log\ \mathrm{softmax}{( \bm{z} )} _ i = z_i - \log \sum _ j \exp(z_j)
  \end{equation}

  Overall, we can approximate the model as:

  % eq 6.31
  \begin{equation} \tag{6.31}
    \label{eq_6_31}
    \mathrm{softmax}{( \bm{z}( \bm{x};\bm{\theta} ) )} _ i \approx
      \frac{\ \sum ^ m _ {j=1} \bm{1} _ {y ^ {(j)} = i, \bm{x} ^ {(j)} = \bm{x}} }
        {\ \sum ^ m _ {j=1} \bm{1} _ {\ \bm{x} ^ {(j)} = \bm{x}} }
  \end{equation}

  Like the sigmoid, the softmax function can saturate if the difference between its inputs is large.

  The softmax function's property:

  % eq 6.32
  \begin{equation} \tag{6.32}
    \label{eq_6_32}
    \mathrm{softmax}( \bm{z} ) = \mathrm{softmax}( \bm{z} + c )
  \end{equation}
  % eq 6.33
  \begin{equation} \tag{6.33}
    \label{eq_6_33}
    \mathrm{softmax}( \bm{z} ) = \mathrm{softmax}( \bm{z} - \max _ i (z _ i) )
  \end{equation}

  With~\eqref{eq_6_32} and~\eqref{eq_6_33}, we can evaluate small difference between each $z _ i$, even if $z _ i$ are large negative number.

  From neuroscience, the softmax becomes a form of \textbf{winner-take-all}.

\subsubsubsection{Other Output Types}
% 3.75p

  Cost function for generalized neural network:
  $p( \bm{y} \ |\ \bm{x};\bm{\theta} )$

  \begin{itemize}

%    \item{Heteroscedastic Model}
%
%    A model to predict
%
%    % eq 6.34
%    \begin{equation} \tag{6.34}
%      \label{eq_6_34}
%      diag(\bm{\beta})
%    \end{equation}

    \item{Mixture Density Networks}

    The Gaussian mixture output is defined as below:

    % eq 6.35
    \begin{equation} \tag{6.35}
      \label{eq_6_35}
      p( \bm{y}\ |\ \bm{x} ) =
        \sum ^ n _ {i=1} p( c=i\ |\ \bm{x} )
        \mathcal{N}( \bm{y}; \bm{\mu} ^ {(i)} (\bm{x}), \bm{\Sigma} ^ {(i)} (\bm{x}) )
    \end{equation}

    The neural network must provide outputs:

    \begin{itemize}

      \item{Mixture components $p(c = i\ |\ \bm{x})$:}

        A multinoulli distribution of $n$ different categories with potential variable $c$.

      \item{Means $\bm{\mu} ^ {(i)} (\bm{x})$:}

        The mean of $i$-th Gaussian distribution.

      \item{Covariances $\bm{\Sigma} ^ {(i)} (\bm{x})$:}

        The covariance matrix of $i$-th Gaussian distribution.

    \end{itemize}

      Using \textbf{clip gradients} in gradient-based optimization on numerically unstable neural networks.

    \newpage

    % fig 6.4
    \setcounter{figure}{3}
    \begin{figure}[ht]
      \begin{center}
        \includegraphics[width=0.8\textwidth]{FIG/fig6_4.PNG}
        \caption{Samples drawn from a neural network with a mixture density output layer.
          The input $x$ is sampled from a uniform distribution,
          and the output $y$ is sampled from $p _ {model} (y\ |\ x)$.
          The neural network is able to learn nonlinear mappings from the input to the parameters of the output distribution.
          These parameters include the probabilities governing which of three mixture components will generate the output as well as the parameters for each mixture component.
          Each mixture component is Gaussian with predicted mean and variance.
          All these aspects of the output distribution are able to vary with respect to the input $x$,
          and to do so in nonlinear ways.
        }\label{fig:NN_mixture_density_output_layer}
      \end{center}
    \end{figure}

    Figure~\ref{fig:NN_mixture_density_output_layer} is an example of mixture density network.

  \end{itemize}

